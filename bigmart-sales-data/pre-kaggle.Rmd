---
title: "big_mart"
author: "Christoper Chan"
date: "January 26, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## TO DO:
1. Create visualizes
2. Model the data



Problem:
Predit the amount of sales of an item.

Approach:
1. Read in and clean data
2. EDA
3. Feature engineering
4. Run a simple linear regression on it
    a) Run a lasso regression to get only the most relevant data
5. Visualize final results

Hypothesis:
1. I think that Item_Visibility and Outlet_Location_Type will have the most predictive power.
2. Item_Identifier will have zero predictive power, it is simply a internal identifier.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(glmnet)
```

## 1 Read and clean data

Reading in the data. Reading in the data with read.csv set the categorical variables correctly as factors, while read_csv set them to char.
```{r}
train <- read.csv('Train.csv')
test <- read.csv('Test.csv')

head(train)
```

Creating a combined df so that I can clean/eda on all the available data
```{r}
test <- mutate(test, Item_Outlet_Sales = 0)

dim(train)
dim(test)
data <- rbind(train, test)

dim(data)
str(data)
```

```{r}
summary(data)
```

I want to see if we have redundant levels for categorical variables. In this case we do, in Item_Fat_Content we have 3 types of low fat and 2 types of regular. I'll standardize it to 'Low Fat' and 'Regular'
```{r}
sapply(data[,2:ncol(data)], levels)
```



Finding missing values. Item weight has 2439 NA. Outlet_Size has 4106 blank instances. Every other variable does not have any missing values. In the grand scheme of things I don't think these variables will have much predictive power so I won't bother to fill them in.
```{r}
sapply(data, function(x) sum(is.na(x)))
sapply(data, function(x) sum(x == ''))
```


Figuring out what to do with the missing values for the Outlet_Size is tricky. It represents a pretty large chunk of the instances. Breaking the data down into levels for each predictor gives us a much clear picture of what's going on. You can use Baye's theorem here, but intuition will be enough here.
```{r}
data %>%
    group_by(Outlet_Location_Type, Outlet_Type, Outlet_Size) %>%
    count()
```

Fixing the blank Outlet_Size. Output verifies that it worked, we no longer have any blank values.
```{r}
data <- data %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 2', 'Small')) %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 3', 'Small'))

sapply(data, function(x) sum(x == ''))
```



```{r}
ggplot(data, aes(Outlet_Type, Outlet_Location_Type, color=Outlet_Size)) +
    geom_point()
```

Now we have to address the NA in Item_Weight. Looking at the distribution of the NA among Item_Types there seems to be some Item_Types that have a larger amount of NA. Note: these values are not normalized so very little inferences can be drawn from this histogram. Instead I'll take the mean of each Item_Type and replace the NA for that type with the mean.
```{r}
a <- data %>%
    group_by(Item_Type) %>%
    filter(is.na(Item_Weight)) %>%
    count()

ggplot(a, aes(Item_Type, n)) +
    geom_bar(stat = 'identity') +
    theme(axis.text.x = element_text(angle=75, vjust=0.7))

b <- data %>%
    group_by(Item_Type) %>%
    summarise_at(vars(Item_Weight), funs(mean(., na.rm=T)))
```
Oh my, so after hours of trying to figure out how to replace all the NAs with the means for that group I came across the answer. 
```{r}
impute_mean <- function(x) replace(x, is.na(x), mean(x, na.rm = T))

data <- data %>%
    group_by(Item_Type) %>%
    mutate(Item_Weight = impute_mean(Item_Weight))
```



## 2
Getting a better sense of the range of data
```{r}
sapply(data, function(x) n_distinct(x))
summary(data)
```

```{r}
ggplot(data, aes(Item_Visibility, Item_Outlet_Sales)) + 
    geom_point(size = 0.75)
```

```{r}
ggplot(data, aes(Item_Type, Item_Outlet_Sales)) +
    geom_bar(stat='identity') +
    theme(axis.text.x = element_text(angle=60, vjust=0.75))
```

Are certain Item_Type given more visibilty than others? No clear connection
```{r}
ggplot(data, aes(Item_Type, Item_Visibility)) +
    geom_boxplot() +
    theme(axis.text.x = element_text(angle=90, vjust=0.8))
```

```{r}
ggplot(train, aes(Item_Visibility, Item_Outlet_Sales, color = Item_Type)) + 
    geom_point(size = 0.75)
```



# 3 Feature Engineering

```{r, message=FALSE}
data <- data %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'LF', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'low fat', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'reg', 'Regular')) %>%
    droplevels()
```

Having an item with zero Item_Visibility that has sales makes little sense. I'm going to make the assumption that people aren't buying items that are stored in the back of the store. Running with Item_Outlet_Sales == 0 yields 0 results, so all the cases of Item_Visibility are beening bought. We'll set the Item_Visibilty for equal to the mean of Item_Visibility.
```{r}
zero_viz <- data %>%
    filter(Item_Visibility == 0 & Item_Outlet_Sales > 0)

avg <- data %>%
    summarize(mean(Item_Visibility))

data <- data %>%
    mutate(Item_Visibility = replace(Item_Visibility, Item_Visibility == 0, 2)) %>%
    mutate(Item_Visibility = as.double(Item_Visibility))

summary(data)
```


Possible to condense the Item_Type down into 3 categories; FD-Food, NC-Non-Consumable, DR:Drink. However I'll hold off on this and see how well the model does. I'm not quite sure how necessary this is.

Note Remove Outlet_Estblishment_Year if not important.
Changing year to years since started.
```{r}
data <- data %>%
    mutate(Years_Open = 2013 -Outlet_Establishment_Year)
data <- data[, c(1,2,3,4,5,6,7,13,9,10,11,12)]
```

Determine the catergory of the item type. I was getting really shit MSE without item_id and outlet id
```{r}
data <- data %>%
    separate(Item_Identifier, c('Item_Category', 'Item_Identifier'), sep=2)
```


## Model
```{r}
mse <- function(x, y) {
    mean((x - y)^2)
}
head(data)
```


Creating the train/test split again
```{r}
set.seed(1)
train <- data[1:8523,]
train_reg <- train

train_samp <- sample(1:nrow(train), nrow(train)*0.7)


test <- train[-train_samp,]
test <- test[,-c(2)]

train <- train[train_samp,]
train <- train[,-c(2)]

real_test <- data[8524:nrow(data),]
```

Baseline model
```{r}
mean_sales <- mean(train$Item_Outlet_Sales)

base1 <- tibble(test$Outlet_Identifier, 'Item_Outlet_Sales' = mean_sales)

mse_base1 <- mse(base1[,2], test[,12])
cat('MSE of baseline model:', mse_base1)
```


Baseline linear model
```{r}
store_lm <- lm(Item_Outlet_Sales~., train)

summary(store_lm)

plot(store_lm)
```

```{r}
testor <- lm(Item_Outlet_Sales ~ Item_MRP + Years_Open + Outlet_Type, train)

summary(testor)
pray_pred <- predict(testor, newx=test)
mse(pray_pred, test[,12])
```

```{r}
lm_pred <- predict(store_lm, newx=test)

mse_lm <- mse(lm_pred, test[,12])
cat('Linear model MSE: ', mse_lm)
```

```{r}
confint(store_lm)
```


```{r}
cor_df <- data %>%
    select_if(is.numeric)
cor_df <- cor_df[-1]

head(cor_df)
cor(cor_df)
```

Creating lambdas which we'll run lasso regression over. Prepparing the data for lasso regression
```{r}
train_reg <- data[1:8523,]
grid <- 10^seq(10, -2, length=100)

mod_mat <- model.matrix(Item_Outlet_Sales~.-Item_Identifier,  train_reg)

train_reg <- mod_mat[train_samp,]
train_y <- train$Item_Outlet_Sales
test_reg <- mod_mat[-train_samp,]
test_y <- test$Item_Outlet_Sales
```


convert factors to dummy variables
```{r}
store_lasso <- glmnet(train_reg, train_y, lambda=grid)

plot(store_lasso)
```

```{r}
store_lasso_cv <- cv.glmnet(train_reg, train_y)

plot(store_lasso_cv)
```

```{r}
best_lam <- store_lasso_cv$lambda.min
cat('Best lambda is:', best_lam)

cvlasso_pred <- predict(store_lasso, s=best_lam, newx=test_reg)

cat('Lasso regularizaion MSE:', mse(cvlasso_pred, test_y))
```

```{r}
cvlasso_coef <- predict(store_lasso, type='coefficients', s=best_lam)
cvlasso_coef

```

```{r}
a <- summary(cvlasso_coef)
b <- tibble(Factor = rownames(cvlasso_coef)[a$i],
       Coefficient = a$x)
b <- b %>%
    mutate(Factor = fct_reorder(Factor, Coefficient))
ggplot(b, aes(Factor, Coefficient)) +
    geom_bar(stat='identity') +
    theme(axis.text.x = element_text(angle=90, vjust=0.75))

```



Things I learned:
lm to some extent fitlers out highly correlated predictors. If I ran years_open and Outlet_Estblishment_year at the same time, Years_Open would become NA because signularities.










