---
title: "big_mart"
author: "Christoper Chan"
date: "January 26, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## TO DO:
1. Replace all NA in item_Weight with the total mean. it doesn't change too much. 
2. Create visualizes
3. Model the data



Problem:
Predit the amount of sales of an item.

Approach:
1. Read in and clean data
2. EDA
3. Feature engineering
4. Run a simple linear regression on it
    a) Run a lasso regression to get only the most relevant data
5. Visualize final results

Hypothesis:
1. I think that Item_Visibility and Outlet_Location_Type will have the most predictive power.
2. Item_Identifier will have zero predictive power, it is simply a internal identifier.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(glmnet)
library(gridExtra)
```

## 1

Reading in the data. Reading in the data with read.csv set the categorical variables correctly as factors, while read_csv set them to char.
```{r}
train <- read.csv('Train.csv')
test <- read.csv('Test.csv')

head(train)
```

Creating a combined df so that I can clean/eda on all the available data
```{r}
test <- mutate(test, Item_Outlet_Sales = 0)

dim(train)
dim(test)
data <- rbind(train, test)

dim(data)
str(data)
```

```{r}
summary(data)
```

I want to see if we have redundant levels for categorical variables. In this case we do, in Item_Fat_Content we have 3 types of low fat and 2 types of regular. I'll standardize it to 'Low Fat' and 'Regular'
```{r}
sapply(data[,2:ncol(data)], levels)
```

```{r}
data <- data %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'LF', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'low fat', 'Low Fat')) %>%
    mutate(Item_Fat_Content = replace(Item_Fat_Content, Item_Fat_Content == 'reg', 'Regular')) %>%
    droplevels()
```

Finding missing values. Item weight has 2439 NA. Outlet_Size has 4106 blank instances. Every other variable does not have any missing values. In the grand scheme of things I don't think these variables will have much predictive power so I won't bother to fill them in.
```{r}
sapply(data, function(x) sum(is.na(x)))
sapply(data, function(x) sum(x == ''))
```

To do:
Use the probabilities of Outlet_Location_Type and Outlet_Type to classify the Outlet_Size.
Figuring out what to do with the missing values for the Outlet_Size is tricky. It represents a pretty large chunk of the instances. Breaking the data down into levels for each predictor gives us a much clear picture of what's going on. You can use Baye's theorem here, but intuition will be enough here.
```{r}
data %>%
    group_by(Outlet_Location_Type, Outlet_Type, Outlet_Size) %>%
    count()
```

Fixing the blank Outlet_Size. Output verifies that it worked, we no longer have any blank values.
```{r}
data <- data %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 2', 'Small')) %>%
    mutate(Outlet_Size = replace(Outlet_Size, Outlet_Size == '' & Outlet_Location_Type == 'Tier 3', 'Small'))

sapply(data, function(x) sum(x == ''))
```



```{r}
ggplot(data, aes(Outlet_Type, Outlet_Location_Type, color=Outlet_Size)) +
    geom_point()
```

Now we have to address the NA in Item_Weight. Looking at the distribution of the NA among Item_Types there seems to be some Item_Types that have a larger amount of NA. Note: these values are not normalized so very little inferences can be drawn from this histogram. Instead I'll take the mean of each Item_Type and replace the NA for that type with the mean.
```{r}
a <- data %>%
    group_by(Item_Type) %>%
    filter(is.na(Item_Weight)) %>%
    count()

ggplot(a, aes(Item_Type, n)) +
    geom_bar(stat = 'identity') +
    theme(axis.text.x = element_text(angle=75, vjust=0.7))

b <- data %>%
    group_by(Item_Type) %>%
    summarise_at(vars(Item_Weight), funs(mean(., na.rm=T)))

b
```


```{r}
test_c <- data %>%
    filter(is.na(Item_Weight)) %>%
    mutate(Item_Weight = replace(Item_Weight, b$Item_Type == Item_Type, b$Item_Weight))

```
```{r}
```

```{r}
a <- ggplot(data, aes(Outlet_Size, Outlet_Location_Type)) +
    geom_point()
b <- ggplot(data, aes(Outlet_Size, Outlet_Type)) +
    geom_point()

grid.arrange(a, b, ncol=2)
```


## 2

Getting a better sense of the range of data
```{r}
sapply(data, function(x) n_distinct(x))
summary(data)
```

```{r}
ggplot(data, aes(Item_Visibility, Item_Outlet_Sales)) + 
    geom_point(size = 0.75)
```

```{r}
ggplot(train, aes(Item_Type, Item_Outlet_Sales)) +
    geom_bar(stat='identity') +
    theme(axis.text.x = element_text(angle=60, vjust=0.75))
```

Are certain Item_Type given more visibilty than others? No clear connection
```{r}
ggplot(train, aes(Item_Type, Item_Visibility)) +
    geom_point(size=0.75) +
    theme(axis.text.x = element_text(angle=90, vjust=0.8))
```

```{r}
ggplot(train, aes(Item_Visibility, Item_Outlet_Sales, color = Item_Type)) + 
    geom_point(size = 0.75)
```


# 3 Feature Engineering

Having an item with zero Item_Visibility that has sales makes little sense. I'm going to make the assumption that people aren't buying items that are stored in the back of the store. Running with Item_Outlet_Sales == 0 yields 0 results, so all the cases of Item_Visibility are beening bought. We'll set the Item_Visibilty for equal to the mean of Item_Visibility.
```{r}
zero_viz <- train %>%
    filter(Item_Visibility == 0 & Item_Outlet_Sales > 0)

avg <- train %>%
    summarize(mean(Item_Visibility))

train <- train %>%
    mutate(Item_Visibility = replace(Item_Visibility, Item_Visibility == 0, 2)) %>%
    mutate(Item_Visibility = as.double(Item_Visibility))

head(train)
```

Possible to condense the Item_Type down into 3 categories; FD-Food, NC-Non-Consumable, DR:Drink. However I'll hold off on this and see how well the model does. I'm not quite sure how necessary this is.

```{r}
zero_vis <- train[train$Item_Visibility] == 0
```


Okay, so I am missing


































